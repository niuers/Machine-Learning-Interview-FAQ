{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Give an example of how the outer product can be useful in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s the geometric interpretation of the dot product of two vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two vectors $u$ and $v$, the dot product is $uv=\\|u\\|\\|v\\|\\cos\\theta$. Where $\\|u\\|\\cos\\theta$ can be regarded as the projection of vector $u$ on vector $v$, so the dot product of $u$ and $v$ is the magnitude of one vector onto another multiplied by the other's magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a vector  u , find vector  v  of unit length such that the dot product of  u  and  v  is maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous question, if $u$ is given, and $v$ has unit length, i.e. $\\|v\\|=1$, the dot product of two is $uv=\\|u\\|\\cos\\theta$, so it's maximized when $\\cos\\theta = 1$, i.e. $\\theta=0$ such that $v$ is in the same direction of $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given two vectors  $a=[3,2,1]$  and  $b=[−1,0,1]$ . Calculate the outer product  $a\\otimes b$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a\\otimes b = ab^T==\\begin{bmatrix}3\\\\2\\\\1\\end{bmatrix}\\begin{bmatrix}-1 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix}-3 & 0 & 3 \\\\ -2 & 0 & 2 \\\\ -1 & 0 & 1\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it mean for two vectors to be linearly independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence of vectors $v_1,\\dots,v_k$ are said to be \"linearly independent\" if the equation $a_1v_1 + \\dots + a_kv_k = 0$ can only be satisfied by $a_i = 0, i=1,\\dots,k$.\n",
    "\n",
    "This implies:\n",
    "1. No vector in the sequence can be represented as a linear combination of the remaining vectors in the sequence.\n",
    "2. The *zero vector* can't be one of \"linearly independent\" vectors.\n",
    "\n",
    "The vectors are said to be \"linearly dependent\" if there exist scalars $a_1, \\dots, a_k$ not all zero, such that\n",
    "$a_1v_1 + \\dots + a_kv_k = 0$. Thus, a set of vectors is linearly dependent if and only if one of them is zero or a linear combination of the others.\n",
    "\n",
    "References:\n",
    "1. [Linearly Independence on Wiki](https://en.wikipedia.org/wiki/Linear_independence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given  $n$  vectors, each of  $d$  dimensions. What is the dimension of their span?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The span of a set of vectors is the set of all linear combinations of the vectors. E.g. for vectors $v_1, \\dots, v_n$, the span is the set of all vectors in the form of $a_1v_1 + \\dots + a_nv_n$, where $a_1, \\dots, a_n$ are any scalars.\n",
    "1. First, [As the dimensionality of a vector refers to the space of which the vector is a member](https://math.stackexchange.com/a/2452453/233623), so the vectors live in a space of dimension $d$ as given. The span of these vectors has to have a dimension less than or equal to $d$. \n",
    "2. Also, the dimension of their span is determined by the number of linearly independent vectors within the $n$ vectors, so it has to be less than or equal to $n$ as well. \n",
    "\n",
    "Combine the two, the dimension of $n$ vectors, each of $d$ dimensions is less than or equal to $min(n,d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given two sets of vectors  $A=a_1,a_2,...,a_n$  and  $B=b_1,b_2,...,b_m$ . How do you check that they share the same basis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, [As the dimensionality of a vector refers to the space of which the vector is a member](https://math.stackexchange.com/a/2452453/233623), the vectors in $A$ and $B$ need to have the same dimension, otherwise, it's impossible for $A, B$ to share the same basis.\n",
    "2. Secondly, a set of vectors can have many bases. We can compute the basis for $A$ first (e.g. solve $Ax=0$ to find out any relationships within the vectors in $A$ and remove extra vectors until we find the basis), assuming it's $C$, then we check that for each $b_i$ in $B$, there's a solution to the equation $Cx=b_i$, i.e. all vectors in $B$ is a linear combination of the basis of $A$. If so, we can say that $A,B$ share the same basis, or they are in the same vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a norm of vector $v$? What is  $l^0$,$l^1$, $l^2$,$l^\\infty norm$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming vector $|v|$ has dimension of $n$, The norm of a nonzero vector $v$ is a positive number $\\|v\\|$, which measures the \"length\" of the vector. Every norm must share two properties of the absolute value $|c|$ for any number(Linear Algebra and Learning from Data, I.11[5]):\n",
    "1. Rescaling: $\\|cv\\| = |c|\\|v\\|$\n",
    "2. Triangle inequality: $\\|v+w\\| \\le \\|v\\|+\\|w\\|$\n",
    "\n",
    "here are some norms:\n",
    "1. [Zero norm, $l^0$ is the number of non-zero elements in the vector](https://www.quora.com/What-is-the-L0-norm-in-linear-algebra)[3]. It is not really a norm as it doesn't satisfy the properties of norm.\n",
    "\n",
    "2. $l^1=\\|v\\|_1 = |v_1| + \\dots + |v_n|$\n",
    "\n",
    "3. $l^2=\\text{Euclidean norm} = \\|v\\|_2 = \\sqrt{|v_1|^2+\\dots+|v_n|^2}$\n",
    "\n",
    "4. $l^\\infty = \\text{max norm} = \\|v\\|_\\infty = \\text{maximum of } |v_1|, \\dots, |v_n|$\n",
    "\n",
    "5. $l^p=(|v_1|^p+\\dots+|v_n|^p)^\\frac{1}{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do norm and metric differ? Given a norm, make a metric. Given a metric, can we make a norm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a metric provides us with a notion of the distance between points in a space, a norm gives us a notion of the length of an individual vector. A norm can only be defined on a vector space, while a metric can be defined on any set. (Introduction to Real Analysis, Metric and Normed Spaces, Christopher Heil).\n",
    "\n",
    "A metric $d$ on a set $X$ needs to satisfy:\n",
    "1. $d(x,y)=0 \\Rightarrow x=y$\n",
    "2. $d(x,y)=d(y,x)$\n",
    "3. $d(x,y)\\le d(x,z) + d(y,z)$\n",
    "\n",
    "[It is easy to see that a norm is a metric on vector space $V$](https://www.quora.com/What-is-the-difference-between-a-metric-and-a-norm/answer/Caleb-Nastasi-1)[4], because length is the same as “distance from 0.”. To check, simply replace vector $v$ by $x-y$, and it's obvious that $d(x,y)=\\|x-y\\|$ satisfies the conditions for a metric in $V$.\n",
    "\n",
    "This does not hold conversely because we may not even have addition of elements in a general set $X$ .\n",
    "\n",
    "Okay, to sum up. All norms are metrics, and normed spaces (vector spaces with a norm) have a lot more structure than general metric spaces. Anything that holds in a metric space will also hold for a normed space. Metric spaces are more general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we say that matrices are linear transformations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linear transformation is defined as a transformation](https://textbooks.math.gatech.edu/ila/linear-transformations.html) $T: R^n \\rightarrow R^m$ satifying [Interactive Linear Algebra, 6]:\n",
    "1. $T(u+v) = T(u) + T(v)$\n",
    "2. $T(cu) = cT(u)$\n",
    "For all vectors $u,v \\in R^n$ and any scalar $c$ . \n",
    "\n",
    "For any matrix $M$, it's easy to verify that $M$ satisfies the requirements of linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What’s the inverse of a matrix? Do all matrices have an inverse? Is the inverse of a matrix always unique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Only square matrices have inverse defined. For a matrix $A_{n \\times n}$, [it is invertable](https://textbooks.math.gatech.edu/ila/matrix-inverses.html) if there exists matrix $B_{n\\times n}$ such that $AB=I_n$ and $BA=I_n$. In such case, $B$ is called the inverse of matrix $A$, and we write $A^{-1} = B$ [6]\n",
    "2. Not all matrices have an inverse, for any square matrix $A_{n\\times n}$, it is invertable is equivalent to any of the following:\n",
    "  * The columns of $A$ are linearly independent. \n",
    "  * The columns of $A$ span $R^n$.\n",
    "  * $Ax=b$ has a unique solution for each $b$ in $R^n$\n",
    "  * The rank of $A$ equals to $n$\n",
    "  * The nullspace of $A$ has rank of $0$, i.e. $N(A)=\\{0\\}$\n",
    "  * $Ax=0$ has only solution of $x=0$\n",
    "  * A square matrix is invertible if and only if $det(A)\\ne 0$.\n",
    "    * Let $A$ be a square matrix. If the rows or columns of $A$ are linearly dependent, then $A$ is not invertible. The rows are dependent. When we reduce the matrix to row echelon form, one or more rows will be zero, then $det(A)=0$.\n",
    "3. The inverse of a (square) matrix is always unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the determinant of a matrix represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of matrix $A$ equals to the product of its eigenvalues.\n",
    "\n",
    "Proof: Let $\\lambda$ be $A$'s eigenvalue, and $x$ be corresponding eigenvectors, then we have $Ax=\\lambda x$, i.e. $(A-\\lambda I)x = 0$, as long as $x!=0$, it indicates that the columns of matrix $A-\\lambda I $ are not linearly independent, thus matrix $A-\\lambda I$ is not invertable, equivalently, $|A-\\lambda I| =  0$, this gives an $nth$ degree polynomial equation for $\\lambda$, which has $n$ roots. Let these roots be $\\lambda_1, \\dots, \\lambda_n$, we have $|A-\\lambda I| = (\\lambda_1 - \\lambda)\\dots(\\lambda_n -\\lambda)$, let $\\lambda = 0$, we have $|A| = \\lambda_1 \\dots \\lambda_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens to the determinant of a matrix if we multiply one of its rows by a scalar  $t×R$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The determinant of the matrix will be scaled by $t$](https://textbooks.math.gatech.edu/ila/determinants-definitions-properties.html). See[Interactive Linear Algebra, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A  $4×4$  matrix has four eigenvalues  $3,3,2,−1$ . What can we say about the trace and the determinant of this matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tr(A) = \\text{sum of eigenvalues} = 3 + 3 + 2 + (-1) = 7$ and $|A| = \\text{product of eigenvalues} = 3*3*2*(-1) = -18$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given the following matrix: Without explicitly using the equation for calculating determinants, what can we say about this matrix’s determinant?\n",
    "$A=\\begin{bmatrix} 1 & 4 & -2 \\\\ -1 & 3 & 2 \\\\ 3 & 5 & -6\\end{bmatrix}$\n",
    "\n",
    "It's clear that column 3 is a multiple of column 1, so the columns are not linearly independent. So $|A| = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Machine Learning Interviews Book, Chip Huyen\n",
    "2. [Github: Machine-Learning-Interview-FAQ](https://github.com/niuers/Machine-Learning-Interview-FAQ)\n",
    "3. https://www.quora.com/What-is-the-L0-norm-in-linear-algebra\n",
    "4. https://www.quora.com/What-is-the-difference-between-a-metric-and-a-norm/answer/Caleb-Nastasi-1\n",
    "5. Linear Algebra and Learning from Data, Gilbert Strang\n",
    "6. [Interactive Linear Algebra](https://textbooks.math.gatech.edu/ila/index.html),Dan Margalit,Joseph Rabinoff,2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
